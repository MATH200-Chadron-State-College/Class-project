
---
title: "Math 200 Project"
author: "Gregory Moses"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F)
options(digits = 3)

library(tidyverse)
library(readr)
student_data<-as.data.frame(read_csv("student_data.csv"))
```

### Data information

This data was collected from my Math 142 (College Algebra) classes. It contains no personally identifiable student information, and the rows have been randomized to further protect student confidentiality.



### Basic data cleaning

Some columns that should be numerical have text entries. We discuss these on a case-by-case basis in the next section, but for now, we will convert this data to numerical data. That will also have the advantage of standardizing my "missing entry" notation; visually inspecting the data as it currently exists, we see both "n/a" and "none" used. We also perform some work with the "Homework" and "Classwork" columns and the Test2 column that we discuss in the "Basic Variable Information" section. There was one "NA" in the final exam column, simply because I didn't bother to enter a "0" at the time; the student was already failing. I fix this.

```{r}
student_data$Test4<-as.numeric(student_data$Test4)
student_data$Homework<-as.numeric(student_data$Homework)
student_data$Classwork<-as.numeric(student_data$Classwork)

Participation<-(select(student_data, Homework,Classwork))
Participation<-rowMeans(Participation, na.rm = TRUE)
student_data<-subset(student_data,select = -c(Homework,Classwork))
student_data<-cbind(student_data,Participation)


student_data$Final[is.na(student_data$Final)] <- 0


test_data<-subset(student_data,select = c(Test1,Test2, Test3, Test4))
test_data$Test2[is.na(test_data$Test2)] <- rowMeans(test_data, na.rm = TRUE)[is.na(test_data$Test2)] 
student_data$Test2<-test_data$Test2


nonzero_data<-student_data[student_data$Final !=0,]

```


### Basic variable information (descriptive)

#### Year

Year: The year of the class. I have data for the following years:

```{r}
print(unique(student_data$Year))
```

#### Semester

The semester. I have data for the following semesters:
```{r}
print(unique(student_data$Semester))
```
In particular, I have never taught this course over the summer.

#### Time

The day and time of day I have taught the class. I have data for the following times:
```{r}
print(unique(student_data$Time))
```
We see three "regular" day/time combinations; two unique entries stemming from our transition to online learning during the outset of COVID; and one unique entry due to a scheduling error that was caught too late to fix.

#### Enrollment

The size of the class.

#### Overall

The student's final grade, measured on a scale from 0 to 100. I refer to this as "overall grade" throughout this document to prevent any confusion between it and the final exam grade.

#### Letter
The student's letter grade. Grades students have received are:
```{r}
print(unique(student_data$Letter))
```
Chadron State does not assign "+" or "-" grades. We see from the data that I have never assigned an Incomplete or other "atypical" grade. Grades are "ranked" in the expected way, F<D<C<B<A

#### Test1, Test2, Test3, Test4

The grades the student received on these tests (out of 100; students may occasionally receive higher grades due to extra credit opportunities). No student has ever legitimately earned a "0" on a test, so a grade of "0" means that the student did not attempt the test. On the other hand, I do not always get to the fourth test; this is reflected as a "NA" in that column.

Analysis done a little later in this document also showed (before data cleaning) two "NA" grades in the Test2 column; this occurred because a student didn't take a test and, for whatever reason, I simply dropped it rather than have them make it up. The effect that had on the data is the same as replacing that "NA" with their test mean, which I did in "Basic data cleaning." I do not want to do that with Test4; there are so many NAs in that column that it would give a very inaccurate picture of the data to get rid of them.


#### Final

The final exam grade; it takes numbers between 0 and 100 (out of 100; students may occasionally receive higher grades due to extra credit opportunities).  No student as legitimately earned a "0" on the final exam, so that grade indicates that the student did not attempt the exam. Note that students who withdrew from the class were removed from my gradebook, and are not included in this data, so all students who did not attempt the final exam, still received a course grade.

#### Participation
In the data, there is a Homework and Classwork column, reflecting different ways I have used to attempt to make students practice the material. They are both recorded out of 100. There have been semesters where I have given one but not the other. 

These categories are extremely fuzzy; for example, in recent semesters, I have given students in-class work that becomes homework if they do not finish it in class. Even when I have taught two classes at the same time with this method, there are times I called this a "homework grade" in one of my gradebooks, and a "classwork grade" in the other gradebook. I also always graded these categories extremely generously. I think it's best to understand both these columns as loosely measuring student participation, and have replaced them with a "Participation" column. This column records the Homework or Classwork grade (in semesters where I have had only one), or their mean (in semesters where I have had both.)

### Elementary statistics

Let's get some baseline information from our data set. 

#### Year

```{r}
year_count<-table(student_data$Year)
barplot(year_count,main = "Year")
```

We see nothing really interesting here. "2018" is artificially stunted because I only have partial data from 2018; 2022 for the same reason. In 2021 we hired Jordan Haas to help reduce faculty overload, and that is reflected in the data.

#### Semester

#### Overall and letter grades

We'll look at our letter grade distribution.

```{r}
grade_count<-table(student_data$Letter)
barplot(grade_count,main = "Letter Grade (all students)")
```

We'll also summarize the overall grades.

```{r}
summary(student_data$Overall)
```





#### Test grades

The exact material covered by each test varies from semester to semester, but are similar enough that it is meaningful to ask about student performance on individual tests.

```{r}
test_data<-subset(student_data,select = c(Test1,Test2, Test3, Test4))
summary(test_data)
for_box<- test_data %>%  pivot_longer(cols=c('Test1', 'Test2','Test3','Test4'),names_to='Test', values_to='Score')
for_box <- for_box %>% na.omit()

ggplot(for_box, aes(x=Test,y=Score)) + geom_boxplot()

```


#### Final Exam Grade

We summarize the final exam grades.

```{r}
summary(student_data$Final)
boxplot(student_data$Final,main = "Final exam scores", ylab = "Scores")
```

#### Participation

We summarize student participation

```{r}
summary(student_data$Participation)
boxplot (student_data$Participation, main = "Student Participation",ylab="Participation grade")
```

### Research question: Are course grades normally distributed?

Outside academia, it is often taken for granted that student grades should fall on a bell curve. I don't know any professors who regularly see this, unless they artificially curve their data. The data from the graph certainly doesn't look normal, but as an illustration of R's capabilities, let's perform the Shapiroâ€“Wilk test.

H_0: The data is normally distributed

H_1: The data is not normally distributed

```{r}
shapiro.test(student_data$Overall)
```
We reject the null hypothesis; these grades are not normally distributed.

### Research question: Do students perform differently on different tests?

The precise material that is on each test varies from semester to semester, but is consistent enough that this is a meaningful question. We start by reproducing our data summary.

```{r}
summary(test_data)
```

Eyeballing this, it looks like performance is similar on the first three tests, but that the fourth test, when there is one, sees better performance.

That's a little deceptive though. By the third test, there are usually students who are no longer participating in the class at all, and therefore take "0"s for skipped tests. We should look at those students, but it's also worth seeing what happens if we only look at students who complete the entire semester.


```{r}
test_data<-subset(student_data,select = c(Test1,Test2, Test3, Test4))
test_nonzero<- filter(test_data, test_data$Test1 > 0, test_data$Test2 > 0, test_data$Test3 > 0, test_data$Test4 > 0)
summary(test_nonzero)
```


We still don't see radical differences between the tests grades, but let's investigate this question further with the Kruskal-Wallis test. We select this test because student test scores are extremely non-normal, making the ANOVA inappropriate. We look at the variances of the test grades to find that the smallest variance was Test 4 (325) and the largest was Test 1 (389); these numbers are close enough that the Kruskal-Wallis test should be appropriate. To do this test, we should lengthen our data.

Let's also remind ourselves of the hypotheses:

H_0: The means of Test 1, Test 2, Test 3, and Test 4 are the same

H_1: At least one of the means is significantly different


```{r}

for_kw<- test_nonzero %>%  pivot_longer(cols=c('Test1', 'Test2','Test3','Test4'),names_to='Test', values_to='Score')

kruskal.test(Score ~ Test, data = for_kw)

```

Although the differences between tests doesn't seem large, the Kruskal-Wallis test suggests that it is significant. Let's investigate this further; are all the tests significantly different? We run the Pairwise Wilcox Test:

H_0: The means of the two categories being compared are the same

H_1: There is a difference between the means

```{r}
pairwise.wilcox.test(for_kw$Score, for_kw$Test,
                 p.adjust.method = "BH")
```

Test 1 and Test 2 are similar to each other, but significantly different from Test 3 and Test 4. Test 3 and Test 4 are similar to each other, but significantly different from Test 1 and Test 2. Looking at the data, we see this; the means for Test 1 and Test 2 are close (76.4 and 74), the means for Test 3 and Test 4 are close (81.7 and 82.8)


### Research question: Is participation correlated to the final exam grade?

Hopefully, student participation is correlated with student success. We select the final exam as a measure of student success (rather than the course grade), because participation is included in the course grade; it's fatuous to talk about them being correlated. We will start by looking for linear correlation, although there's no particular reason it should be linear.

```{r}

ggplot(student_data, aes(Participation, Final)) +    geom_point()
cor(student_data$Participation,student_data$Final)
```

We see what would normally be thought of as a moderate correlation (0.5<r<0.7). 

<!-- Looking at the data, students who give up before they take the final exam will obviously not be impacted by Participation (i.e. they will get a 0 automatically), hence the points on the horizontal axis. Let's try removing those from the picture. We do so with the understanding that we will look at students who did not take the final exam separately. -->

```{r}
# library(devtools)
# install_github("ProcessMiner/nlcor")
# ggplot(nonzero_data, aes(Participation, Final)) +    geom_point()
# 
# nonzero_cor<- nlcor(nonzero_data$Participation,nonzero_data$Final,plt = T)
# 
# nonzero_cor$cor.estimate
```

