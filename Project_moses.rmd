
---
title: "Math 200 Project"
author: "Gregory Moses"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F)
options(digits = 3)

library(tidyverse)
library(readr)
student_data<-as.data.frame(read_csv("student_data.csv"))
```

### Data information

This data was collected from my Math 142 (College Algebra) classes. It contains no personally identifiable student information, and the rows have been randomized to further protect student confidentiality.  299 students are represented in the data.




### Basic data cleaning

Some columns that should be numerical have text entries. We discuss these on a case-by-case basis in the next section, but for now, we will convert this data to numerical data. That will also have the advantage of standardizing my "missing entry" notation; visually inspecting the data as it currently exists, we see both "n/a" and "none" used. We also perform some work with the "Homework" and "Classwork" columns and the Test2 column that we discuss in the "Basic Variable Information" section. There was one "NA" in the final exam column, simply because I didn't bother to enter a "0" at the time; the student was already failing. I fix this.

```{r}
student_data$Test4<-as.numeric(student_data$Test4)
student_data$Homework<-as.numeric(student_data$Homework)
student_data$Classwork<-as.numeric(student_data$Classwork)

Participation<-(select(student_data, Homework,Classwork))
Participation<-rowMeans(Participation, na.rm = TRUE)
student_data<-subset(student_data,select = -c(Homework,Classwork))
student_data<-cbind(student_data,Participation)


student_data$Final[is.na(student_data$Final)] <- 0


test_data<-subset(student_data,select = c(Test1,Test2, Test3, Test4))
test_data$Test2[is.na(test_data$Test2)] <- rowMeans(test_data, na.rm = TRUE)[is.na(test_data$Test2)] 
student_data$Test2<-test_data$Test2


nonzero_data<-student_data[student_data$Final !=0,]

```


### Basic variable information (descriptive)

We briefly discuss the data we have collected.

#### Year

Year: The year of the class. 

#### Semester

The semester. 

#### Time

The day and time of day I have taught the class. 

#### Enrollment

The size of the class.

#### Overall

The student's final grade, measured on a scale from 0 to 100. I refer to this as "overall grade" throughout this document to prevent any confusion between it and the final exam grade.

#### Letter
The student's letter grade. Grades students have received are:
```{r}
print(unique(student_data$Letter))
```
Chadron State does not assign "+" or "-" grades. We see from the data that I have never assigned an Incomplete or other "atypical" grade. Grades are "ranked" in the expected way, F<D<C<B<A

#### Test1, Test2, Test3, Test4

The grades the student received on these tests (out of 100; students may occasionally receive higher grades due to extra credit opportunities). No student has ever legitimately earned a "0" on a test, so a grade of "0" means that the student did not attempt the test. On the other hand, I do not always get to the fourth test; this is reflected as a "NA" in that column.

Analysis done a little later in this document also showed (before data cleaning) two "NA" grades in the Test2 column; this occurred because a student didn't take a test and, for whatever reason, I simply dropped it rather than have them make it up. The effect that had on the data is the same as replacing that "NA" with their test mean, which I did in "Basic data cleaning." I do not want to do that with Test4; there are so many NAs in that column that it would give a very inaccurate picture of the data to get rid of them.


#### Final

The final exam grade; it takes numbers between 0 and 100 (out of 100; students may occasionally receive higher grades due to extra credit opportunities).  No student as legitimately earned a "0" on the final exam, so that grade indicates that the student did not attempt the exam. Note that students who withdrew from the class were removed from my gradebook, and are not included in this data, so all students who did not attempt the final exam, still received a course grade.

#### Participation
In the data, there is a Homework and Classwork column, reflecting different ways I have used to attempt to make students practice the material. They are both recorded out of 100. There have been semesters where I have given one but not the other. 

These categories are extremely fuzzy; for example, in recent semesters, I have given students in-class work that becomes homework if they do not finish it in class. Even when I have taught two classes at the same time with this method, there are times I called this a "homework grade" in one of my gradebooks, and a "classwork grade" in the other gradebook. I also always graded these categories extremely generously. I think it's best to understand both these columns as loosely measuring student participation, and have replaced them with a "Participation" column. This column records the Homework or Classwork grade (in semesters where I have had only one), or their mean (in semesters where I have had both.)

### Elementary statistics

Let's get some baseline information from our data set. 

#### Year
I have data for the following years:

```{r}
print(unique(student_data$Year))
```

However, my data from 2018 is stunted; changes in Sakai have made my earliest gradebooks unavailable to me. My data from 2022 is incomplete because I am including no data from the current (Fall) semester.

```{r}
year_count<-table(student_data$Year)
barplot(year_count,main = "Year")
```

We see nothing really interesting here. 2018 and 2022 are artificially shortened as discussed. In 2021 we hired an additional faculty member to help reduce faculty overload, and that is reflected in the data.

#### Semester

I have data for the following semesters:
```{r}
print(unique(student_data$Semester))
```
In particular, I have never taught this course over the summer. We will look at student enrollment by semester; the following data comes entirely from the years 2019 - 2021, so that I have data about both semesters.

```{r}
for_semester_count<- filter(student_data, student_data$Year > 2018, student_data$Year < 2022)
semester_count<-table(for_semester_count$Semester)
barplot(semester_count,main = "Number of students by semester")
print(semester_count)
```

#### Time

I have taught during the following times:
```{r}
print(unique(student_data$Time))
```
We see three "regular" day/time combinations; two unique entries stemming from our transition to online learning during the outset of COVID; and one unique entry due to a scheduling error that was caught too late to fix.

#### Enrollment

```{r}
summary(student_data$Enrollment)
boxplot (student_data$Enrollment, main = "Student Enrollment",ylab="Number of students enrolled in class")
```

#### Overall and letter grades

We'll look at our letter grade distribution.

```{r}
grade_count<-table(student_data$Letter)
barplot(grade_count,main = "Letter Grade (all students)")
```

We'll also summarize the overall grades.

```{r}
summary(student_data$Overall)
```





#### Test grades and final exam grade

The exact material covered by each test varies from semester to semester, but are similar enough that it is meaningful to ask about student performance on individual tests.

```{r}
test_data<-subset(student_data,select = c(Test1,Test2, Test3, Test4,Final))
summary(test_data)
for_box<- test_data %>%  pivot_longer(cols=c('Test1', 'Test2','Test3','Test4','Final'),names_to='Test', values_to='Score')
for_box <- for_box %>% na.omit()

ggplot(for_box, aes(x=Test,y=Score)) + geom_boxplot()

```



#### Participation

We summarize student participation

```{r}
summary(student_data$Participation)
boxplot (student_data$Participation, main = "Student Participation",ylab="Participation grade")
```

### Research question: Are course grades normally distributed?

Outside academia, it is often taken for granted that student grades should fall on a bell curve. I don't know any professors who regularly see this, unless they artificially curve their data. The data from the graph certainly doesn't look normal, but as an illustration of R's capabilities, let's perform the Shapiroâ€“Wilk test.

H_0: The data is normally distributed

H_1: The data is not normally distributed

```{r}
shapiro.test(student_data$Overall)
```
We reject the null hypothesis; these grades are not normally distributed.

### Research question: Do students perform differently on different tests?

The precise material that is on each test varies from semester to semester, but is consistent enough that this is a meaningful question. We start by reproducing our data summary.

```{r}
summary(test_data)
```

Eyeballing this, it looks like performance is similar on the first three tests, but that the fourth test, when there is one, sees better performance, and the Final Exam sees worse.

Let's investigate this question further with the Kruskal-Wallis test. We select this test because student test scores are extremely non-normal, making the ANOVA inappropriate. We look at the variances of the test grades to find that the smallest variance was Test 4 (325) and the largest was Test 1 (389); these numbers are close enough that the Kruskal-Wallis test (p=0.05) should be appropriate. To do this test, we should lengthen our data.



Let's also remind ourselves of the hypotheses:

H_0: The means of Test 1, Test 2, Test 3, and Test 4 are the same

H_1: At least one of the means is significantly different


```{r}

for_kw<- test_data %>%  pivot_longer(cols=c('Test1', 'Test2','Test3','Test4','Final'),names_to='Test', values_to='Score')

kruskal.test(Score ~ Test, data = for_kw)

```

The Kruskal-Wallis test suggests that significant differences exist. Let's investigate this further; are all the tests significantly different? We run the Pairwise Wilcox Test (p=0.05):

H_0: The means of the two categories being compared are the same

H_1: There is a difference between the means

```{r}
pairwise.wilcox.test(for_kw$Score, for_kw$Test,
                 p.adjust.method = "BH")
```

The mean of the final test is significantly different from the means of the hourly tests, except for Test 2. The mean of the fourth test, when there is one, is significantly different from all the other means except for Test 3. Otherwise we do not see statistically significant differences between the means.

### Research question: Is participation linearly correlated to the final exam grade?

Hopefully, student participation is correlated with student success. We select the final exam as a measure of student success (rather than the course grade), because participation is included in the course grade; it's fatuous to talk about them being correlated. We will look for linear correlation, although there's no particular reason it should be linear.

```{r}

ggplot(student_data, aes(Participation, Final)) +    geom_point()
cor(student_data$Participation,student_data$Final)
```

We see what would normally be thought of as a moderate correlation (0.5<r<0.7). 

### Research question: Does the time of day a class is taught impact student performance?

It is a sort of academic urban legend that students will perform better when the class is taught earlier, because students are tired and not paying attention late in the day. Let's look at the three "normal" times that I teach college algebra, and see what proportion of students receive each letter grade.

```{r}
time_vs_grade <- subset(student_data, Time=='MTWR 200-250' | Time=="MW 100-150, TR 1230-0120" | Time== "MTWR 100-150")

time_vs_grade$Letter<-factor(time_vs_grade$Letter)
time_vs_grade$Time<-factor(time_vs_grade$Time)

ggplot(data = time_vs_grade, aes(x=Time, fill=Letter) ) + geom_bar(position="fill")
```

We very clearly see that classes taught from 1 - 2 see much better student performance than classes taught from 2 - 3, in terms of having significantly more A's, and significantly fewer F's. On the other hand, the class with inconsistent meeting times has the worst performance of all, so it's not as simple as saying that earlier classes are better. Interestingly, the proportion of "C" students seems to to be the same across the three standard class times.

### Research question: Does the semester a class is taught impact student performance?

It's another academic urban legend that students will do worse in the Fall than the Spring. The exact rational for this is murky, but we can easily check whether it is reflected in my grades.

```{r}
ggplot(data = time_vs_grade, aes(x=Semester, fill=Letter) ) + geom_bar(position="fill")
```

### Research question: Have student grades improved as I have gained experience as a professor?

#### Letter grade distribution

Let's start by looking at grade distributions over time.

```{r}
ggplot(data = student_data, aes(x=Year, fill=Letter) ) + geom_bar(position="fill")
```

This is heartening; after a rough year (2019), the number of F's I have given has trended clearly downwards, and the number of A's I have given has trended upwards.

#### Overall grade

Let's look at this graphically.

```{r}
grades_by_year<-student_data
grades_by_year$Year<-as.character(grades_by_year$Year)
ggplot(grades_by_year, aes(x=Year, y=Overall) ) + geom_boxplot()
```
Since 2019, the course mean has trended up, the quartiles have trended up, and the outliers (representing students who did exceptionally poorly), are going down. We saw this in the previous analysis as well, but so far, 2021 has been very slightly better than the current year, at least in terms of some metrics. At the moment, this doesn't trouble me; some fluctuations are to be expected. 